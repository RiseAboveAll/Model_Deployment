{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model-Deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jTus26p4kScL",
        "QkRYkMyUolaj",
        "uZej4F3JvZ2I"
      ],
      "authorship_tag": "ABX9TyNK7S/g8RfWbBxD38MDbPE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiseAboveAll/Model_Deployment/blob/master/Model_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTus26p4kScL",
        "colab_type": "text"
      },
      "source": [
        "# Model Deployment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqZ2k5QYkW1Y",
        "colab_type": "text"
      },
      "source": [
        "<h1> Agenda </h1>\n",
        "\n",
        "- When we develop a model in jupyter notebook , model is in the RAM of the system, the moment we close jupyter notebook model is gone , hence we need to retrain which is not viable. Hence we need to save model in the disk. It is called **Serialization of Machine Learning Model**.\n",
        "\n",
        "- Productionising model in real time . In production one record come in at a time . We expose mode as Rest API.\n",
        "\n",
        "- We give model to testing team , they might say model does not run their . Packaging the model for reproducability . Docker sets virtual enviornment, where along with your model, dependencies for model will also be packaged, we just give files which defines the particular environment. \n",
        "\n",
        "- Pipeline\n",
        "\n",
        "- Scaling - Instead of one request , what if 100 request come at same time "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkRYkMyUolaj",
        "colab_type": "text"
      },
      "source": [
        "# Model Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ivThRvOoocn",
        "colab_type": "text"
      },
      "source": [
        "- Putting the model in server , enabling it to take input data inorder to return an output is model deployment\n",
        "\n",
        "- Ensure data from multiple system .\n",
        "\n",
        "- Fetching records from outer source\n",
        "\n",
        "When we deploy model we can have : \n",
        "\n",
        "1. Batch Mode for Deploying Model : \n",
        "\n",
        "  We store all information in database and run the model till that point again. Hence we score for all the record till the latest being entered in the record. \n",
        "\n",
        "2. Real Time for Deploying Model :\n",
        "\n",
        "  Instant response for the data you gave to model. It require infrastructure for responding milisecond. \n",
        "\n",
        "You need to save your model , the process is called **Pickeling / Serialization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZej4F3JvZ2I",
        "colab_type": "text"
      },
      "source": [
        "# Model Serialization - Pickling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbldP-GQvbp5",
        "colab_type": "text"
      },
      "source": [
        "library used to save model is pickle, **pickle.dump(model-name)** is the command to save model onto the disk. **pickle.load** is the command to load a model from the disk on to the memory. To score the new record you would use pickle.load() , it will load the model and then predict the score .\n",
        "\n",
        "\n",
        "```\n",
        "import os\n",
        "import pickle\n",
        "filename='himanshu.pras'\n",
        "#It store the model in the given filename and write the content of the model in write binary mode.\n",
        "\n",
        "pickle.dump(model,open(filename,'wb')) \n",
        "\n",
        "# Load Model in read binary mode :\n",
        "loaded_model=pickle.load(open(filename,'rb'))\n",
        "result=loaded_model.predict(X_test)\n",
        "\n",
        "```\n",
        "\n",
        "Alternate Method :\n",
        "\n",
        "```\n",
        "from sklearn.externals import jobilb\n",
        "filename='himanshu.sav'\n",
        "joblib.dump(model,filename)\n",
        "loaded_model=joblib.load(filename)\n",
        "result=loaded_model.predict(X_test)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osMR8Owe0yt6",
        "colab_type": "text"
      },
      "source": [
        "# Updatable Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSkkaJc901qG",
        "colab_type": "text"
      },
      "source": [
        "- To make model continuously learn . It is based on incremental learning .\n",
        "\n",
        "- We split dataset in multiple parts and keep training model incrementaly, chunk by chunk. Model will continuously update weights as per the chunk.\n",
        "\n",
        "- This can be used for streaming data.\n",
        "\n",
        "- Incremental classifiers are :\n",
        "\n",
        "  - MultinomialNB\n",
        "\n",
        "  - BernoulliNB\n",
        "\n",
        "  - Perceptron\n",
        "\n",
        "  - SGDClassifier\n",
        "\n",
        "<h1>Incremental Classifier</h1>\n",
        "\n",
        "```\n",
        "from sklearn import linear_model\n",
        "chunksize=25\n",
        "for chunk in pd.read_csv('data.csv',header=None,chunksize=chunksize):\n",
        "  train_sub=chunk\n",
        "  y=train_sub.iloc[:,4]\n",
        "  X=train_sub.drop([4],axis=1)\n",
        "  clf=linear_model.SGDClassifier()\n",
        "  clf.partial_fit(X,y,classes=np.unique(y))\n",
        "  pred=clf.predict(test)\n",
        "  print(accuracy_score(pred,labeld_test))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBfpfdsfg-NL",
        "colab_type": "text"
      },
      "source": [
        "# Batch Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnXk3CdphGQo",
        "colab_type": "text"
      },
      "source": [
        "- We have task schedulers , we pick the script of python and run at given particular time. In this we load the python scrit while loading the model via pickle file. \n",
        "\n",
        "- We have a source data , we process the data and then make it available to the model whose pickle file is loaded. Model will make prediction for new records and print it in database .\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP4JXkMslZ68",
        "colab_type": "text"
      },
      "source": [
        "# Real Time Prediction \n",
        "\n",
        "<img src=https://raw.githubusercontent.com/RiseAboveAll/Model_Deployment/master/Real_Time_Mode.PNG>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu3l-vQsrdNH",
        "colab_type": "text"
      },
      "source": [
        "- ReactJS is the front end, it sends request to server.  \n",
        "\n",
        "- Backend APIs are provided as Micro-Services. It consumes data from database also. On the Server end it processes the request and respond to ReactJS.\n",
        "\n",
        "- ReactJS is the client, and rest are the server components which process your data received from the client. \n",
        "\n",
        "- As soon as the submit button is clicked on the client end, you will get the real time prediction. As client will be able to send the data to the server where the model is loaded or exposed and client will get response immediately. \n",
        "\n",
        "- Things to think about : \n",
        "\n",
        "  - How many servers to manage?\n",
        "\n",
        "  - How to manage the workload?\n",
        "\n",
        "- In server end your model runs as Rest API. It keeps listening and waiting for any requests that comes from the client. \n",
        "\n",
        "<h1>Server :</h1>\n",
        "\n",
        "Steps :\n",
        "\n",
        "1. Go to Command Prompt\n",
        "\n",
        "2. Go to Directory where your server script is stored , use : cd path\n",
        "\n",
        "3. You will find server.py file in the directory \n",
        "\n",
        "4. Run 'python server.py' command\n",
        "\n",
        "<h1>Client :</h1>\n",
        "\n",
        "Steps :\n",
        "\n",
        "1. Go to Command Prompt\n",
        "\n",
        "2. Go to Directory where your client script is stored , use : cd path\n",
        "\n",
        "3. You will find client.py file in the directory \n",
        "\n",
        "4. Run 'python client.py' command\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evrx5eQqBlg8",
        "colab_type": "text"
      },
      "source": [
        "<h1>server.py</h1>\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle \n",
        "from flask import Flask,request,jsonify\n",
        "\n",
        "os.chdir(path-where-pickle-is)\n",
        "#Load the model\n",
        "model=pickle.load(open('model-filename.sav','rb'))\n",
        "app=Flask(__name__)\n",
        "@app.route('/api',methods=['POST'])\n",
        "def predict():\n",
        "  #Get data from POST request\n",
        "  data=request.get_json(force=True)\n",
        "  # Make Prediction using model loaded as per the data\n",
        "  predict_request=[[data['sl'],data['sw'],data['pl'],data['pw']]\n",
        "  predict_request=np.array(predict_request)\n",
        "  prediction=model.predict(predict_request)\n",
        "  #Take the first value of prediction\n",
        "  output=prediction[0]\n",
        "  print(output)\n",
        "  return jsonify(int(output))\n",
        "\n",
        "if __name__=='__main__':\n",
        "  app.run(port=8011,debug=True)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9d7GfjFhve",
        "colab_type": "text"
      },
      "source": [
        "<h1>client.py</h1>\n",
        "\n",
        "```\n",
        "import requests\n",
        "import json\n",
        "url='http://localhost:8011/api'\n",
        "data=json.dumps({'sl':3,'sw':2,'pl':1,'pw':5})\n",
        "r=requests.post(url,data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogq6BKYLF10o",
        "colab_type": "text"
      },
      "source": [
        "# Docker Concentration - Development Enviornment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAhTqXzhHXxs",
        "colab_type": "text"
      },
      "source": [
        "Each model have its own different dependencies. Some can be using pandas different version. \n",
        "\n",
        "Using Docker :\n",
        "\n",
        "- There will be no enviroment issues\n",
        "\n",
        "- No OS issues\n",
        "\n",
        "- Preconfigured Environment\n",
        "\n",
        "- Build once and run anywhere with docker\n",
        "\n",
        "\n",
        "Like Github we also have Dockerhub hub.docker.com.. Dockerhub is something where lot of image is available (model image). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYR17e2xrXWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVxgc95v0xJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_C7BNRoIhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuK6Rd_LkRer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}