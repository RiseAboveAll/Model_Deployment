{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model-Deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jTus26p4kScL",
        "QkRYkMyUolaj",
        "uZej4F3JvZ2I",
        "osMR8Owe0yt6",
        "EBfpfdsfg-NL",
        "WP4JXkMslZ68"
      ],
      "authorship_tag": "ABX9TyO90ej/lmnAUMPTChLA2pFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiseAboveAll/Model_Deployment/blob/master/Model_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTus26p4kScL",
        "colab_type": "text"
      },
      "source": [
        "# Model Deployment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqZ2k5QYkW1Y",
        "colab_type": "text"
      },
      "source": [
        "<h1> Agenda </h1>\n",
        "\n",
        "- When we develop a model in jupyter notebook , model is in the RAM of the system, the moment we close jupyter notebook model is gone , hence we need to retrain which is not viable. Hence we need to save model in the disk. It is called **Serialization of Machine Learning Model**.\n",
        "\n",
        "- Productionising model in real time . In production one record come in at a time . We expose mode as Rest API.\n",
        "\n",
        "- We give model to testing team , they might say model does not run their . Packaging the model for reproducability . Docker sets virtual enviornment, where along with your model, dependencies for model will also be packaged, we just give files which defines the particular environment. \n",
        "\n",
        "- Pipeline\n",
        "\n",
        "- Scaling - Instead of one request , what if 100 request come at same time "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkRYkMyUolaj",
        "colab_type": "text"
      },
      "source": [
        "# Model Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ivThRvOoocn",
        "colab_type": "text"
      },
      "source": [
        "- Putting the model in server , enabling it to take input data inorder to return an output is model deployment\n",
        "\n",
        "- Ensure data from multiple system .\n",
        "\n",
        "- Fetching records from outer source\n",
        "\n",
        "When we deploy model we can have : \n",
        "\n",
        "1. Batch Mode for Deploying Model : \n",
        "\n",
        "  We store all information in database and run the model till that point again. Hence we score for all the record till the latest being entered in the record. \n",
        "\n",
        "2. Real Time for Deploying Model :\n",
        "\n",
        "  Instant response for the data you gave to model. It require infrastructure for responding milisecond. \n",
        "\n",
        "You need to save your model , the process is called **Pickeling / Serialization**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZej4F3JvZ2I",
        "colab_type": "text"
      },
      "source": [
        "# Model Serialization - Pickling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbldP-GQvbp5",
        "colab_type": "text"
      },
      "source": [
        "library used to save model is pickle, **pickle.dump(model-name)** is the command to save model onto the disk. **pickle.load** is the command to load a model from the disk on to the memory. To score the new record you would use pickle.load() , it will load the model and then predict the score .\n",
        "\n",
        "\n",
        "```\n",
        "import os\n",
        "import pickle\n",
        "filename='himanshu.pras'\n",
        "#It store the model in the given filename and write the content of the model in write binary mode.\n",
        "\n",
        "pickle.dump(model,open(filename,'wb')) \n",
        "\n",
        "# Load Model in read binary mode :\n",
        "loaded_model=pickle.load(open(filename,'rb'))\n",
        "result=loaded_model.predict(X_test)\n",
        "\n",
        "```\n",
        "\n",
        "Alternate Method :\n",
        "\n",
        "```\n",
        "from sklearn.externals import jobilb\n",
        "filename='himanshu.sav'\n",
        "joblib.dump(model,filename)\n",
        "loaded_model=joblib.load(filename)\n",
        "result=loaded_model.predict(X_test)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osMR8Owe0yt6",
        "colab_type": "text"
      },
      "source": [
        "# Updatable Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSkkaJc901qG",
        "colab_type": "text"
      },
      "source": [
        "- To make model continuously learn . It is based on incremental learning .\n",
        "\n",
        "- We split dataset in multiple parts and keep training model incrementaly, chunk by chunk. Model will continuously update weights as per the chunk.\n",
        "\n",
        "- This can be used for streaming data.\n",
        "\n",
        "- Incremental classifiers are :\n",
        "\n",
        "  - MultinomialNB\n",
        "\n",
        "  - BernoulliNB\n",
        "\n",
        "  - Perceptron\n",
        "\n",
        "  - SGDClassifier\n",
        "\n",
        "<h1>Incremental Classifier</h1>\n",
        "\n",
        "```\n",
        "from sklearn import linear_model\n",
        "chunksize=25\n",
        "for chunk in pd.read_csv('data.csv',header=None,chunksize=chunksize):\n",
        "  train_sub=chunk\n",
        "  y=train_sub.iloc[:,4]\n",
        "  X=train_sub.drop([4],axis=1)\n",
        "  clf=linear_model.SGDClassifier()\n",
        "  clf.partial_fit(X,y,classes=np.unique(y))\n",
        "  pred=clf.predict(test)\n",
        "  print(accuracy_score(pred,labeld_test))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBfpfdsfg-NL",
        "colab_type": "text"
      },
      "source": [
        "# Batch Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnXk3CdphGQo",
        "colab_type": "text"
      },
      "source": [
        "- We have task schedulers , we pick the script of python and run at given particular time. In this we load the python scrit while loading the model via pickle file. \n",
        "\n",
        "- We have a source data , we process the data and then make it available to the model whose pickle file is loaded. Model will make prediction for new records and print it in database .\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP4JXkMslZ68",
        "colab_type": "text"
      },
      "source": [
        "# Real Time Prediction \n",
        "\n",
        "<img src=https://raw.githubusercontent.com/RiseAboveAll/Model_Deployment/master/Real_Time_Mode.PNG>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu3l-vQsrdNH",
        "colab_type": "text"
      },
      "source": [
        "- ReactJS is the front end, it sends request to server.  \n",
        "\n",
        "- Backend APIs are provided as Micro-Services. It consumes data from database also. On the Server end it processes the request and respond to ReactJS.\n",
        "\n",
        "- ReactJS is the client, and rest are the server components which process your data received from the client. \n",
        "\n",
        "- As soon as the submit button is clicked on the client end, you will get the real time prediction. As client will be able to send the data to the server where the model is loaded or exposed and client will get response immediately. \n",
        "\n",
        "- Things to think about : \n",
        "\n",
        "  - How many servers to manage?\n",
        "\n",
        "  - How to manage the workload?\n",
        "\n",
        "- In server end your model runs as Rest API. It keeps listening and waiting for any requests that comes from the client. \n",
        "\n",
        "<h1>Server :</h1>\n",
        "\n",
        "Steps :\n",
        "\n",
        "1. Go to Command Prompt\n",
        "\n",
        "2. Go to Directory where your server script is stored , use : cd path\n",
        "\n",
        "3. You will find server.py file in the directory \n",
        "\n",
        "4. Run 'python server.py' command\n",
        "\n",
        "<h1>Client :</h1>\n",
        "\n",
        "Steps :\n",
        "\n",
        "1. Go to Command Prompt\n",
        "\n",
        "2. Go to Directory where your client script is stored , use : cd path\n",
        "\n",
        "3. You will find client.py file in the directory \n",
        "\n",
        "4. Run 'python client.py' command\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evrx5eQqBlg8",
        "colab_type": "text"
      },
      "source": [
        "<h1>server.py</h1>\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle \n",
        "from flask import Flask,request,jsonify\n",
        "\n",
        "os.chdir(path-where-pickle-is)\n",
        "#Load the model\n",
        "model=pickle.load(open('model-filename.sav','rb'))\n",
        "app=Flask(__name__)\n",
        "@app.route('/api',methods=['POST'])\n",
        "def predict():\n",
        "  #Get data from POST request\n",
        "  data=request.get_json(force=True)\n",
        "  # Make Prediction using model loaded as per the data\n",
        "  predict_request=[[data['sl'],data['sw'],data['pl'],data['pw']]\n",
        "  predict_request=np.array(predict_request)\n",
        "  prediction=model.predict(predict_request)\n",
        "  #Take the first value of prediction\n",
        "  output=prediction[0]\n",
        "  print(output)\n",
        "  return jsonify(int(output))\n",
        "\n",
        "if __name__=='__main__':\n",
        "  app.run(port=8011,debug=True)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9d7GfjFhve",
        "colab_type": "text"
      },
      "source": [
        "<h1>client.py</h1>\n",
        "\n",
        "```\n",
        "import requests\n",
        "import json\n",
        "url='http://localhost:8011/api'\n",
        "data=json.dumps({'sl':3,'sw':2,'pl':1,'pw':5})\n",
        "r=requests.post(url,data)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogq6BKYLF10o",
        "colab_type": "text"
      },
      "source": [
        "# Docker Concentration - Development Enviornment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAhTqXzhHXxs",
        "colab_type": "text"
      },
      "source": [
        "Each model have its own different dependencies. Some can be using pandas different version. \n",
        "\n",
        "Using Docker :\n",
        "\n",
        "- There will be no enviroment issues\n",
        "\n",
        "- No OS issues\n",
        "\n",
        "- Preconfigured Environment\n",
        "\n",
        "- Build once and run anywhere with docker\n",
        "\n",
        "\n",
        "Like Github we also have Dockerhub hub.docker.com.. Dockerhub is something where lot of image is available (model image). \n",
        "\n",
        "Steps : \n",
        "\n",
        "1. Create a docker folder\n",
        "\n",
        "2. In folder you need to have a docker file and docker-requirements.txt\n",
        "\n",
        "3. Docker file consist of commands to be executed in docker \n",
        "\n",
        "```\n",
        "FROM python:3\n",
        "RUN apt-get update && apt-get install -y python3-pip\n",
        "COPY requirements.txt\n",
        "RUN pip install -r requirements.txt\n",
        "#Install Jupyter\n",
        "RUN pip3 install jupyter\n",
        "#Create a new system user\n",
        "RUN useradd -ms /bin/bash demo\n",
        "#Change to this new user\n",
        "USER demo\n",
        "#Set container working directory to the user home folder\n",
        "WORKDIR /home/demo\n",
        "#Start Jupyter Notebook\n",
        "ENTRYPOINT [\"jupyter\",\"notebook\",\"--ip=0.0.0.0\"]\n",
        "```\n",
        "In requirements.txt file it contains the library and their versions\n",
        "\n",
        "To check docker Images : docker images \n",
        "\n",
        "To create Docker :\n",
        "\n",
        "- Go to command prompt, change your directory to the disk where your docker file and requirements.txt is located\n",
        "\n",
        "- To create new image of python along with the requred libraries : write command : docker build -t <name-of-image> . (This dot(.) means that files are located in the current directory.\n",
        "\n",
        "- docker images\n",
        "\n",
        "- To run the image : docker run -it <name-of-image>\n",
        "\n",
        "- This won't run jupyter notebook in interactive session we need to give another command for it \n",
        "\n",
        "- To shut down docker press cntrl+c \n",
        "\n",
        "\n",
        "- To run jupyter notebook : docker run -p <port-number : 8888> <name-of-image>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTvVT-HOsuxp",
        "colab_type": "text"
      },
      "source": [
        "# Docker Concentration - Productionization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4H72GtBs4od",
        "colab_type": "text"
      },
      "source": [
        "<h1>Docker File</h1>\n",
        "\n",
        "```\n",
        "#Install the Image which has Python 3.6 , Flask and nginx server installed\n",
        "\n",
        "> FROM tiangolo/uwsgi-nginx-flask:python3.6\n",
        "\n",
        ">WORKDIR /app/\n",
        "\n",
        ">COPY requirements.txt /app/\n",
        "\n",
        ">RUN pip install -r ./requirements.txt \n",
        "\n",
        "#Set the environment name as production\n",
        "\n",
        ">ENV ENVIRONMENT production\n",
        "\n",
        "#Execute the App, i.e execute a server\n",
        "\n",
        ">COPY main.py __init__.py /app/\n",
        "```\n",
        "<h1>requirements.txt</h1>\n",
        "\n",
        "```\n",
        "numpy==1.13\n",
        "scipy==0.19.1\n",
        "Flask==0.12.2\n",
        "pandas==0.20.2\n",
        "scikit_learn==0.18.2\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "- You need to create empty __init__.py file .\n",
        "\n",
        "- You need to create main.py file which is the server file\n",
        "\n",
        "<h1>main.py</h1>\n",
        "\n",
        "```\n",
        "import os\n",
        "from flask import Flask,request\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "from sklearn import datasets\n",
        "import pickle\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import ensemble \n",
        "from sklearn import model_selection\n",
        "\n",
        "iris=datasets.load_iris()\n",
        "train=iris.data\n",
        "labels=iris.target\n",
        "\n",
        "#creating and saving model\n",
        "df Ensemble():\n",
        "  rf = ensemble.RandomForestClassifier(n_estimators=500)\n",
        "  rf.fit(train,label)\n",
        "  pickle.dump(rf,open('iris.pkl','wb')\n",
        "\n",
        "Ensemble()\n",
        "\n",
        "app=Flask(__name__)\n",
        "@app.route('/isAlive')\n",
        "\n",
        "def index():\n",
        "  return \"true\"\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def get_prediction():\n",
        "  #Works only for single sample\n",
        "  data=request.get_json() #Get Data posted As Json\n",
        "  data=np.array(data)[np.newaxis,:] #Converts shape from (4,) to (1,4)\n",
        "  model = pickle.load(open('iris.pkl','rb'))\n",
        "  prediction=model.predict(data)#runs globally loaded model on the data\n",
        "  return str(prediction[0])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  if os.environ['ENVIRONMENT']=='production' :\n",
        "    app.run(port=80,host='0.0.0.0')\n",
        "\n",
        "```\n",
        "\n",
        "Now since all the files are ready , we need to build a docker image. Steps are as follows :\n",
        "\n",
        "- Change your path to these file located disk : cd path\n",
        "\n",
        "- Build image using the files in the path : docker build -t <image-name> . \n",
        "\n",
        "- Map port in Docker & Run Server : docker run -p 3456:80 <image-name>\n",
        "\n",
        "- Open new CMD window : curl -X POST 0.0.0.0:3456/predict -H 'Content-Type: application/json' -d '[8,7,7,6]'\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJVTboPDB98C",
        "colab_type": "text"
      },
      "source": [
        "# Kubernetes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1g_W6OLCBjh",
        "colab_type": "text"
      },
      "source": [
        "- It is about how to productionalize your model on cloud\n",
        "\n",
        "- Used for dealing with the situation when lot many request are coming , hence kubernetes cluster auto cluster your models. We port docker image on to kubernetes cluster which auto scale your nodes based on workload that is coming.\n",
        "\n",
        "- It will provide the load balancer inorder to distribute the load  across the nodes.\n",
        "\n",
        "- It also take care of fault tolerence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIEwazC11CIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYR17e2xrXWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVxgc95v0xJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_C7BNRoIhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuK6Rd_LkRer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}